{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "from lime import lime_tabular\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "def preprocessing(fake=pd.read_csv(\"True.csv\"),true=pd.read_csv(\"True.csv\")):\n",
    "    lenfake=len(fake)\n",
    "    lentrue=len(true)\n",
    "    fake['target'] = 'fake'\n",
    "    true['target'] = 'true'\n",
    "    data = pd.concat([fake, true]).reset_index(drop = True)\n",
    "    data.drop([\"date\"],axis=1,inplace=True)\n",
    "    data.drop([\"title\"],axis=1,inplace=True)\n",
    "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "    def punctuation_removal(text):\n",
    "        all_list = [char for char in text if char not in string.punctuation]\n",
    "        clean_str = ''.join(all_list)\n",
    "        return clean_str\n",
    "\n",
    "    data['text'] = data['text'].apply(punctuation_removal)\n",
    "    nltk.download('stopwords')\n",
    "    stop = stopwords.words('english')\n",
    "    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    print('data lenght = ',len(data))\n",
    "    print('lenfake = ',lenfake)\n",
    "    print('lentrue = ',lentrue)\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataproc(data,nbfake,nbtrue):\n",
    "    print(data)\n",
    "    if nbfake>=(lenfake-1):\n",
    "        print('Not enough fake news in dataset')\n",
    "    if nbtrue>=(lentrue-1):\n",
    "        print('Not enough true news in dataset')\n",
    "\n",
    "    a=random.sample(range(0, lenfake), nbfake)\n",
    "    b=random.sample(range(lenfake, (lenfake+lentrue-1)), nbtrue)\n",
    "    print(b)\n",
    "    d=data.iloc[a][:]\n",
    "    e=data.iloc[b][:]\n",
    "    data2=pd.concat([d,e], axis=0)\n",
    "\n",
    "    X_train,X_test,y_train,y_test = train_test_split(data2['text'],data2.target,test_size=0.2,random_state=42)\n",
    "\n",
    "    return(X_train,X_test,y_train,y_test)\n",
    "\n",
    "#data2=dataproc(preprocessing(),nbfake,nbtrue)\n",
    "#print(data2[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def subjects(data):\\n    print(data.groupby([\\'subject\\'])[\\'text\\'].count())\\n    data.groupby([\\'subject\\'])[\\'text\\'].count().plot(kind=\"bar\")\\n    plt.show()\\n\\nsubjects(dataproc(data2,13,14))'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def subjects(data):\n",
    "    print(data.groupby(['subject'])['text'].count())\n",
    "    data.groupby(['subject'])['text'].count().plot(kind=\"bar\")\n",
    "    plt.show()\n",
    "\n",
    "subjects(dataproc(data2,13,14))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_t =  (10,)\n",
      "37422    washington reuters taliban increased amount te...\n",
      "18043    rome reuters italy traditional political parti...\n",
      "24297    beijing reuters chinese president xi jinping u...\n",
      "17888    tokyo reuters japan yuriko koike says went pol...\n",
      "34466    mexico city reuters mexican soldiers arbitrari...\n",
      "7432     jakartaislamabadcairo reuters many muslims aro...\n",
      "18671    washington reuters us president donald trump e...\n",
      "28637    beijing reuters whether presidentelect donald ...\n",
      "29723    reuters lifesized nude statue us republican pr...\n",
      "20846    cape town reuters south africa top prosecutor ...\n",
      "Name: text, dtype: object\n",
      "(array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.03395049, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.02463937, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ]]), array([[0.        , 0.07726427, 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.        , 0.        , 0.05982098, ..., 0.        , 0.        ,\n",
      "        0.29910488]]), TfidfVectorizer(lowercase=False))\n"
     ]
    }
   ],
   "source": [
    "def vecto(X_train,X_test):\n",
    "    \n",
    "    X_t=pd.concat([X_train,X_test])\n",
    "    print('X_t = ',X_t.shape)\n",
    "    ltotal=len(X_t)\n",
    "    print(X_t)\n",
    "    vectorizer = TfidfVectorizer(lowercase=False)\n",
    "    v = vectorizer.fit_transform(X_t)\n",
    "    '''\n",
    "    print('X_t = ',X_t)\n",
    "    print('X_t type = ',type(X_t))\n",
    "    print('X_t shape = ', X_t.shape)\n",
    "    print('X_t[4] = ',X_t[4])\n",
    "    print('X_t[4] type = ',type(X_t[4]))\n",
    "    #print('v = ',v)\n",
    "    print('v shape = ', v.shape)\n",
    "'''\n",
    "    features=vectorizer.get_feature_names_out()\n",
    "    ltrain=len(X_train)\n",
    "    vtrain=v[0:ltrain][:]\n",
    "    vtest=v[ltrain:][:]\n",
    "    vtrain=scipy.sparse.csr_matrix.toarray(vtrain)\n",
    "    vtest=scipy.sparse.csr_matrix.toarray(vtest)\n",
    "    '''\n",
    "    print(type(vtest))\n",
    "    print('vtest = ',vtest)\n",
    "    print('vtest shape = ',vtest.shape)\n",
    "    print('FEATURES = ',features)\n",
    "    print('length features = ', len(features))\n",
    "'''\n",
    "    return(vtrain,vtest,vectorizer)\n",
    "    \n",
    "\n",
    "print(vecto(data2[0],data2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function classifLogReg at 0x7fbcb09714c0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def classifLogReg(vtrain,vtest,y_train,y_test):\n",
    "    mo=LogisticRegression()\n",
    "    mo.fit(vtrain,y_train)\n",
    "    m=mo.predict(vtest)\n",
    "    m=np.array(m)\n",
    "    y_test=np.array(y_test)\n",
    "    print(y_test)\n",
    "    print(m)\n",
    "\n",
    "    print(\"accuracy: {}%\".format(round(accuracy_score(y_test, m)*100,2)))\n",
    "    return(round(accuracy_score(y_test, m)*100,2))\n",
    "print(classifLogReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef p(news):\\n    print(\\'news = \\',news)\\n    print(\"news type = \",type(news))\\n    news=pd.Series(news)\\n    v3=vectorizer.transform(news)\\n\\n    print(\\'v3 = \\',v3)\\n    print(\\'fonction p lancée\\')\\n\\n    v3=scipy.sparse.csr_matrix.toarray(v3)\\n    print(\\'v3 shape = \\',v3.shape)\\n    r3=mo.predict(v3)\\n    print(\\'r3 = \\',r3)\\n    return(r3)\\n\\n\\n#xk=p([\\'test\\'])\\n#print(X_train[2].split(\" \"))\\n#x=p(X_train[2].split(\" \"))\\nx=p(X_train[2])\\n#print(X_train[2])\\n#print(np.shape(x))\\n#print((X_train[2].split(\" \")))\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def p(news):\n",
    "    print('news = ',news)\n",
    "    print(\"news type = \",type(news))\n",
    "    news=pd.Series(news)\n",
    "    v3=vectorizer.transform(news)\n",
    "\n",
    "    print('v3 = ',v3)\n",
    "    print('fonction p lancée')\n",
    "\n",
    "    v3=scipy.sparse.csr_matrix.toarray(v3)\n",
    "    print('v3 shape = ',v3.shape)\n",
    "    r3=mo.predict(v3)\n",
    "    print('r3 = ',r3)\n",
    "    return(r3)\n",
    "\n",
    "\n",
    "#xk=p(['test'])\n",
    "#print(X_train[2].split(\" \"))\n",
    "#x=p(X_train[2].split(\" \"))\n",
    "x=p(X_train[2])\n",
    "#print(X_train[2])\n",
    "#print(np.shape(x))\n",
    "#print((X_train[2].split(\" \")))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classifNaiveBayes(vtrain,y_train,vectorizer,X_test):\n",
    "    mo=MultinomialNB()\n",
    "    mo.fit(vtrain,y_train)\n",
    "\n",
    "    print(type(X_test))\n",
    "\n",
    "    #print('X_test = ',X_test)\n",
    "    #print('X_test type = ',type(X_test))\n",
    "    print('xtest index = ',X_test[:][0])\n",
    "    i=X_test[:][0] \n",
    "    j=i[2]\n",
    "    o=pd.Series(X_test[j])\n",
    "    o=vectorizer.transform(o)\n",
    "    o=scipy.sparse.csr_matrix.toarray(o)\n",
    "    o=o[0]\n",
    "    '''\n",
    "    print('o = ', o)\n",
    "    print('o type = ',type(o))\n",
    "    print('o shape = ',o.shape)\n",
    "\n",
    "\n",
    "    print('vtrain shape = ',vtrain.shape)\n",
    "    print('vtrain type = ',type(vtrain))\n",
    "    print('vtrain[5][5] = ',vtrain[5][5])\n",
    "    print('vtrain[5][5] type = ',type(vtrain[5][5]))\n",
    "    '''\n",
    "    explainer = lime_tabular.LimeTabularExplainer(vtrain,mode=\"classification\",categorical_features=features,feature_names=features)\n",
    "    exp=explainer.explain_instance(data_row=o,predict_fn=mo.predict_proba, num_features=20) #labels=features\n",
    "    exp.as_pyplot_figure()\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.tight_layout()\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(features)\\nwith plt.style.context(\"ggplot\"):\\n    fig = plt.figure(figsize=(8,6))\\n    plt.barh(range(len(mo.coef_[0])), mo.coef_[0], color=[\"red\" if coef<0 else \"green\" for coef in mo.coef_[0]])\\n    plt.yticks(range(len(mo.coef_[0])), features);\\n    plt.title(\"Weights\")\\n    '"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(features)\n",
    "with plt.style.context(\"ggplot\"):\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.barh(range(len(mo.coef_[0])), mo.coef_[0], color=[\"red\" if coef<0 else \"green\" for coef in mo.coef_[0]])\n",
    "    plt.yticks(range(len(mo.coef_[0])), features);\n",
    "    plt.title(\"Weights\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/evan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data lenght =  42834\n",
      "lenfake =  21417\n",
      "lentrue =  21417\n",
      "                                                    text       subject target\n",
      "0      washington reuters head conservative republica...  politicsNews   fake\n",
      "1      washington reuters transgender people allowed ...  politicsNews   fake\n",
      "2      washington reuters special counsel investigati...  politicsNews   fake\n",
      "3      washington reuters trump campaign adviser geor...  politicsNews   fake\n",
      "4      seattlewashington reuters president donald tru...  politicsNews   fake\n",
      "...                                                  ...           ...    ...\n",
      "42829  brussels reuters nato allies tuesday welcomed ...     worldnews   true\n",
      "42830  london reuters lexisnexis provider legal regul...     worldnews   true\n",
      "42831  minsk reuters shadow disused sovietera factori...     worldnews   true\n",
      "42832  moscow reuters vatican secretary state cardina...     worldnews   true\n",
      "42833  jakarta reuters indonesia buy 11 sukhoi fighte...     worldnews   true\n",
      "\n",
      "[42834 rows x 3 columns]\n",
      "[36892, 28559, 29907, 40242, 36239]\n",
      "X_t =  (10,)\n",
      "36892    seoul reuters ten north koreans including four...\n",
      "22182    beijing reuters white house wednesday condemne...\n",
      "29907    fort lauderdale fla reuters republican preside...\n",
      "9062     washington reuters three months ago sarah ibra...\n",
      "36239    berlin reuters housing crisis spreading german...\n",
      "12158    united nations reuters united nations secretar...\n",
      "20543    dakar reuters togo must go way west african na...\n",
      "28559    lima reuters us president barack obama sunday ...\n",
      "40242    cairo reuters egypt said thursday air force hi...\n",
      "18428    vienna reuters austrian chancellor christian k...\n",
      "Name: text, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2131\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2140\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St Cyr/EMSCC-FakeNewsProject/LIME.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000014?line=7'>8</a>\u001b[0m     vtrain,vtest,vectorizer\u001b[39m=\u001b[39mvecto(X_train,X_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000014?line=8'>9</a>\u001b[0m     classifNaiveBayes(vtrain,y_train,vectorizer,X_test)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000014?line=10'>11</a>\u001b[0m limenaivebayes(\u001b[39m5\u001b[39;49m,\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St Cyr/EMSCC-FakeNewsProject/LIME.ipynb Cell 10'\u001b[0m in \u001b[0;36mlimenaivebayes\u001b[0;34m(nbfake, nbtrue)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000014?line=6'>7</a>\u001b[0m y_test\u001b[39m=\u001b[39mx1[\u001b[39m3\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000014?line=7'>8</a>\u001b[0m vtrain,vtest,vectorizer\u001b[39m=\u001b[39mvecto(X_train,X_test)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000014?line=8'>9</a>\u001b[0m classifNaiveBayes(vtrain,y_train,vectorizer,X_test)\n",
      "\u001b[1;32m/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St Cyr/EMSCC-FakeNewsProject/LIME.ipynb Cell 8'\u001b[0m in \u001b[0;36mclassifNaiveBayes\u001b[0;34m(vtrain, y_train, vectorizer, X_test)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000012?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(X_test))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000012?line=6'>7</a>\u001b[0m \u001b[39m#print('X_test = ',X_test)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000012?line=7'>8</a>\u001b[0m \u001b[39m#print('X_test type = ',type(X_test))\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000012?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mxtest index = \u001b[39m\u001b[39m'\u001b[39m,X_test[:][\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000012?line=9'>10</a>\u001b[0m i\u001b[39m=\u001b[39mX_test[:][\u001b[39m0\u001b[39m] \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/LIME.ipynb#ch0000012?line=10'>11</a>\u001b[0m j\u001b[39m=\u001b[39mi[\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=954'>955</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=956'>957</a>\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=957'>958</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=959'>960</a>\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=960'>961</a>\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=961'>962</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=962'>963</a>\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=1065'>1066</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=1067'>1068</a>\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=1068'>1069</a>\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/series.py?line=1069'>1070</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def limenaivebayes(nbfake,nbtrue):\n",
    "    data=preprocessing()\n",
    "    x1=dataproc(data,nbfake,nbtrue)\n",
    "    X_train=x1[0]\n",
    "    X_test=x1[1]\n",
    "    y_train=x1[2]\n",
    "    y_test=x1[3]\n",
    "    vtrain,vtest,vectorizer=vecto(X_train,X_test)\n",
    "    classifNaiveBayes(vtrain,y_train,vectorizer,X_test)\n",
    "    \n",
    "limenaivebayes(5,5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52fd7cfe315652c26f116941759022c1154f5724bf85b7c7e9d121c272da0755"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
