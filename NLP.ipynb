{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6\n",
    "\n",
    "NMF (Non-negative Matrix Factorization): \n",
    "A->W+H avec \n",
    "A=articles by words, original\n",
    "H=Article by topics, topics found\n",
    "W=topics by words, weight of these topics\n",
    "\n",
    "\n",
    "NMF is more scalable than LDA, but LDA more frequently used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the new system “Canton becomes Guangzhou and Tientsin becomes Tianjin.” Most importantly, the newspaper would now refer to the country’s capital as Beijing, not Peking. This was a step too far for some American publications. In an article on Pinyin around this time, the Chicago Tribune said that while it would be adopting the system for most Chinese words, some names had “become so ingrained\n",
      "['new', 'system', 'canton', 'becom', 'guangzhou', 'tientsin', 'becom', 'tianjin', 'import', 'newspap', 'would', 'refer', 'countri', 'capit', 'beij', 'peke', 'step', 'far', 'american', 'public', 'articl', 'pinyin', 'around', 'time', 'chicago', 'tribun', 'said', 'would', 'adopt', 'system', 'chines', 'word', 'name', 'becom', 'ingrain']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/evan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Text cleaning\n",
    "\n",
    "punc=string.punctuation\n",
    "nltk.download('stopwords')\n",
    "stops=set(stopwords.words('english'))\n",
    "#print(stops)\n",
    "\n",
    "\n",
    "def process_text(t):\n",
    "    t=casual_tokenize(t)\n",
    "    \n",
    "    t=[e.lower() for e in t]\n",
    "    \n",
    "    t=[re.sub('[0-9]+', '',e) for e in t]\n",
    "    \n",
    "    t=[contractions.fix(e) for e in t]\n",
    "    \n",
    "    t=[SnowballStemmer('english').stem(e) for e in t]\n",
    "    t=[w for w in t if w not in punc]\n",
    "    t=[w for w in t if w not in stops]\n",
    "    t=[e for e in t if len(e)>1]\n",
    "    t=[e for e in t if ' ' not in e]\n",
    "    \n",
    "    return t\n",
    "\n",
    "\n",
    "text = 'In the new system “Canton becomes Guangzhou and Tientsin becomes Tianjin.” Most importantly, the newspaper would now refer to the country’s capital as Beijing, not Peking. This was a step too far for some American publications. In an article on Pinyin around this time, the Chicago Tribune said that while it would be adopting the system for most Chinese words, some names had “become so ingrained'\n",
    "print(text)\n",
    "text_cleaned=process_text(text)\n",
    "print(text_cleaned)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "text1 0         new\n",
      "      1      system\n",
      "      2      canton\n",
      "      3       becom\n",
      "      4   guangzhou\n",
      "...             ...\n",
      "text2 30     chines\n",
      "      31       word\n",
      "      32       name\n",
      "      33      becom\n",
      "      34    ingrain\n",
      "\n",
      "[70 rows x 1 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only join an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St Cyr/EMSCC-FakeNewsProject/NLP.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(texts)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=5'>6</a>\u001b[0m tfidf_vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=6'>7</a>\u001b[0m     min_df\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, \u001b[39m#on ne retient pas les mots apparaissant dans moins de n textes différents\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=7'>8</a>\u001b[0m     max_df\u001b[39m=\u001b[39m\u001b[39m0.85\u001b[39m, \u001b[39m#on ignore les mots apparaissant dans plus de 0,n% des articles : non représentatifs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=10'>11</a>\u001b[0m     preprocessor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=11'>12</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=13'>14</a>\u001b[0m tfidf \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39;49mfit_transform(texts)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2078\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2070'>2071</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2071'>2072</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2072'>2073</a>\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2073'>2074</a>\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2074'>2075</a>\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2075'>2076</a>\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2076'>2077</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2077'>2078</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2078'>2079</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2079'>2080</a>\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2080'>2081</a>\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1329'>1330</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1330'>1331</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1331'>1332</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1332'>1333</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1333'>1334</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1334'>1335</a>\u001b[0m             )\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1335'>1336</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1337'>1338</a>\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1339'>1340</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1340'>1341</a>\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1206'>1207</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1207'>1208</a>\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1208'>1209</a>\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1209'>1210</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1210'>1211</a>\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=108'>109</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=109'>110</a>\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=110'>111</a>\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=111'>112</a>\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=112'>113</a>\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only join an iterable"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "\n",
    "texts = pd.DataFrame(text_cleaned)\n",
    "texts=pd.concat([texts,texts],keys=['text1','text2'])\n",
    "print(texts)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=3, #on ne retient pas les mots apparaissant dans moins de n textes différents\n",
    "    max_df=0.85, #on ignore les mots apparaissant dans plus de 0,n% des articles : non représentatifs\n",
    "    max_features=5000, #nombre de mots max\n",
    "    ngram_range=(1, 2),\n",
    "    preprocessor=' '.join\n",
    ")\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52fd7cfe315652c26f116941759022c1154f5724bf85b7c7e9d121c272da0755"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
