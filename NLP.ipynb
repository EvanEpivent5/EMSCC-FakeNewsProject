{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6\n",
    "\n",
    "NMF (Non-negative Matrix Factorization): \n",
    "A->W+H avec \n",
    "A=articles by words, original\n",
    "H=Article by topics, topics found\n",
    "W=topics by words, weight of these topics\n",
    "\n",
    "\n",
    "NMF is more scalable than LDA, but LDA more frequently used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'system', 'canton', 'becom', 'guangzhou', 'tientsin', 'becom', 'tianjin', 'import', 'newspap', 'would', 'refer', 'countri', 'capit', 'beij', 'peke', 'step', 'far', 'american', 'public', 'articl', 'pinyin', 'around', 'time', 'chicago', 'tribun', 'said', 'would', 'adopt', 'system', 'chines', 'word', 'name', 'becom', 'ingrain']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/evan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Text cleaning\n",
    "\n",
    "punc=string.punctuation\n",
    "nltk.download('stopwords')\n",
    "stops=set(stopwords.words('english'))\n",
    "#print(stops)\n",
    "\n",
    "\n",
    "def process_text(t):\n",
    "    t=casual_tokenize(t)\n",
    "    \n",
    "    t=[e.lower() for e in t]\n",
    "    \n",
    "    t=[re.sub('[0-9]+', '',e) for e in t]\n",
    "    \n",
    "    t=[contractions.fix(e) for e in t]\n",
    "    \n",
    "    t=[SnowballStemmer('english').stem(e) for e in t]\n",
    "    t=[w for w in t if w not in punc]\n",
    "    t=[w for w in t if w not in stops]\n",
    "    t=[e for e in t if len(e)>1]\n",
    "    t=[e for e in t if ' ' not in e]\n",
    "    \n",
    "    return t\n",
    "\n",
    "\n",
    "#text = 'In the new system “Canton becomes Guangzhou and Tientsin becomes Tianjin.” Most importantly, the newspaper would now refer to the country’s capital as Beijing, not Peking. This was a step too far for some American publications. In an article on Pinyin around this time, the Chicago Tribune said that while it would be adopting the system for most Chinese words, some names had “become so ingrained'\n",
    "#print(text)\n",
    "#text_cleaned=process_text(text)\n",
    "print(text_cleaned)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'concat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St Cyr/EMSCC-FakeNewsProject/NLP.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=0'>1</a>\u001b[0m \u001b[39m# Data processing\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=2'>3</a>\u001b[0m texts \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(text_cleaned)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=3'>4</a>\u001b[0m texts\u001b[39m.\u001b[39;49mconcat(texts)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(texts)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=5'>6</a>\u001b[0m tfidf_vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=6'>7</a>\u001b[0m     min_df\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, \u001b[39m#on ne retient pas les mots apparaissant dans moins de n textes différents\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=7'>8</a>\u001b[0m     max_df\u001b[39m=\u001b[39m\u001b[39m0.85\u001b[39m, \u001b[39m#on ignore les mots apparaissant dans plus de 0,n% des articles : non représentatifs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=10'>11</a>\u001b[0m     preprocessor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/evan/Library/CloudStorage/OneDrive-Personal/Stage/St%20Cyr/EMSCC-FakeNewsProject/NLP.ipynb#ch0000012?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/generic.py?line=5567'>5568</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/generic.py?line=5568'>5569</a>\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/generic.py?line=5569'>5570</a>\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/generic.py?line=5570'>5571</a>\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/generic.py?line=5571'>5572</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/generic.py?line=5572'>5573</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/generic.py?line=5573'>5574</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> <a href='file:///Users/evan/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/pandas/core/generic.py?line=5574'>5575</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'concat'"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "\n",
    "texts = pd.DataFrame(text_cleaned)\n",
    "texts.concat(texts)\n",
    "print(texts)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=3, #on ne retient pas les mots apparaissant dans moins de n textes différents\n",
    "    max_df=0.85, #on ignore les mots apparaissant dans plus de 0,n% des articles : non représentatifs\n",
    "    max_features=5000, #nombre de mots max\n",
    "    ngram_range=(1, 2),\n",
    "    preprocessor=' '.join\n",
    ")\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52fd7cfe315652c26f116941759022c1154f5724bf85b7c7e9d121c272da0755"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
