{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME\n",
    "\n",
    "https://towardsdatascience.com/interpreting-image-classification-model-with-lime-1e7064a2f2e5\n",
    "\n",
    "LIME : Local Interpretable Model-Agnostic Explanations\n",
    "Independant of the Model\n",
    "\n",
    "LIME tries to find the explanation of your black-box model by approximating the local linear behavior of your model\n",
    "Warning : Local interpretation only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapes\n",
    "1 - Génération de datas artificiels très proches de l'input en modifiant légèrement les valeurs dans la zone d'intérêt\n",
    "\n",
    "2 - Prédiction de la classe des points modifiés\n",
    "\n",
    "3 - Calcul du poids de chaque point artificiel : distance métrique cosine par rapport à l'input de base. Plus le point artificiel est proche de l'image de base, plus son poids est important\n",
    "\n",
    "4 - Regression linéaire avec les points artificiels pondérés : on détecte quels points ont le rôle important dans la prédiction du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemples LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP\n",
    "\n",
    "https://shap.readthedocs.io/en/latest/index.html\n",
    "\n",
    "SHapley Additive exPlanations\n",
    "\n",
    "Connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions\n",
    "\n",
    "En résumé, les valeurs de Shapley calculent l’importance d’une variable en comparant ce qu’un modèle prédit avec et sans cette variable. Cependant, étant donné que l’ordre dans lequel un modèle voit les variables peut affecter ses prédictions, cela se fait dans tous les ordres possibles, afin que les fonctionnalités soient comparées équitablement. Cette approche est inspirée de la théorie des jeux.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemples SHAP\n",
    "\n",
    "https://github.com/slundberg/shap/blob/master/notebooks/tabular_examples/model_agnostic/Diabetes%20regression.ipynb\n",
    "\n",
    "https://medium.com/@ulalaparis/repousser-les-limites-dexplicabilité-un-guide-avancé-de-shap-a33813a4bbfc\n",
    "\n",
    "https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME vs. SHAP\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52fd7cfe315652c26f116941759022c1154f5724bf85b7c7e9d121c272da0755"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
