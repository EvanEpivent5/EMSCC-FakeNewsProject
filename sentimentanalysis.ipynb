{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "from lime import lime_tabular\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/evan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text       subject target\n",
      "0      washington reuters head conservative republica...  politicsNews   fake\n",
      "1      washington reuters transgender people allowed ...  politicsNews   fake\n",
      "2      washington reuters special counsel investigati...  politicsNews   fake\n",
      "3      washington reuters trump campaign adviser geor...  politicsNews   fake\n",
      "4      seattlewashington reuters president donald tru...  politicsNews   fake\n",
      "...                                                  ...           ...    ...\n",
      "42829  brussels reuters nato allies tuesday welcomed ...     worldnews   true\n",
      "42830  london reuters lexisnexis provider legal regul...     worldnews   true\n",
      "42831  minsk reuters shadow disused sovietera factori...     worldnews   true\n",
      "42832  moscow reuters vatican secretary state cardina...     worldnews   true\n",
      "42833  jakarta reuters indonesia buy 11 sukhoi fighte...     worldnews   true\n",
      "\n",
      "[42834 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "def preprocessing(fake=pd.read_csv(\"True.csv\"),true=pd.read_csv(\"True.csv\")):\n",
    "    fake['target'] = 'fake'\n",
    "    true['target'] = 'true'\n",
    "    data = pd.concat([fake, true]).reset_index(drop = True)\n",
    "    data.drop([\"date\"],axis=1,inplace=True)\n",
    "    data.drop([\"title\"],axis=1,inplace=True)\n",
    "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "    def punctuation_removal(text):\n",
    "        all_list = [char for char in text if char not in string.punctuation]\n",
    "        clean_str = ''.join(all_list)\n",
    "        return clean_str\n",
    "\n",
    "    data['text'] = data['text'].apply(punctuation_removal)\n",
    "    nltk.download('stopwords')\n",
    "    stop = stopwords.words('english')\n",
    "    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #print('data lenght = ',len(data))\n",
    "    #print('lenfake = ',lenfake)\n",
    "    #print('lentrue = ',lentrue)\n",
    "   \n",
    "    return(data)\n",
    "\n",
    "d=preprocessing()\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataproc(data,nbfake,nbtrue,lenfake,lentrue):\n",
    "    #print(data)\n",
    "    if nbfake>=(lenfake-1):\n",
    "        print('Not enough fake news in dataset')\n",
    "    if nbtrue>=(lentrue-1):\n",
    "        print('Not enough true news in dataset')\n",
    "    print(len(data))\n",
    "    a=random.sample(range(0, lenfake), nbfake)\n",
    "    b=random.sample(range(lenfake+1, len(data)-1), nbtrue)\n",
    "    #print(b)\n",
    "    print(' b = ',b)\n",
    "    d=data.iloc[a][:]\n",
    "    print(\" a = \", a)\n",
    "\n",
    "    print(\"data = \",data)\n",
    "    e=data.iloc[b][:]\n",
    "    \n",
    "    data2=pd.concat([d,e], axis=0)\n",
    "\n",
    "    X_train,X_test,y_train,y_test = train_test_split(data2['text'],data2.target,test_size=0.2,random_state=42)\n",
    "\n",
    "    return(X_train,X_test,y_train,y_test)\n",
    "\n",
    "#data2=dataproc(preprocessing(),nbfake,nbtrue)\n",
    "#print(data2[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def subjects(data):\\n    print(data.groupby([\\'subject\\'])[\\'text\\'].count())\\n    data.groupby([\\'subject\\'])[\\'text\\'].count().plot(kind=\"bar\")\\n    plt.show()\\n\\nsubjects(dataproc(data2,13,14))'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def subjects(data):\n",
    "    print(data.groupby(['subject'])['text'].count())\n",
    "    data.groupby(['subject'])['text'].count().plot(kind=\"bar\")\n",
    "    plt.show()\n",
    "\n",
    "subjects(dataproc(data2,13,14))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecto(X_train,X_test):\n",
    "    \n",
    "    X_t=pd.concat([X_train,X_test])\n",
    "    #print('X_t = ',X_t.shape)\n",
    "    ltotal=len(X_t)\n",
    "    #print(X_t)\n",
    "    vectorizer = TfidfVectorizer(lowercase=False)\n",
    "    v = vectorizer.fit_transform(X_t)\n",
    "\n",
    "    features=vectorizer.get_feature_names_out()\n",
    "    ltrain=len(X_train)\n",
    "    vtrain=v[0:ltrain][:]\n",
    "    vtest=v[ltrain:][:]\n",
    "    vtrain=scipy.sparse.csr_matrix.toarray(vtrain)\n",
    "    vtest=scipy.sparse.csr_matrix.toarray(vtest)\n",
    "\n",
    "    return(vtrain,vtest,vectorizer,features)\n",
    "    \n",
    "\n",
    "#print(vecto(data2[0],data2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function classifLogReg at 0x7fe5497a0700>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def classifLogReg(vtrain,vtest,y_train,y_test):\n",
    "    mo=LogisticRegression()\n",
    "    mo.fit(vtrain,y_train)\n",
    "    m=mo.predict(vtest)\n",
    "    m=np.array(m)\n",
    "    y_test=np.array(y_test)\n",
    "    #print(y_test)\n",
    "    #print(m)\n",
    "\n",
    "    #print(\"accuracy: {}%\".format(round(accuracy_score(y_test, m)*100,2)))\n",
    "    return(round(accuracy_score(y_test, m)*100,2))\n",
    "print(classifLogReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classifNaiveBayes(vtrain,y_train,vectorizer,X_test,features):\n",
    "    mo=MultinomialNB()\n",
    "    mo.fit(vtrain,y_train)\n",
    "\n",
    "    i=X_test.index\n",
    "    j=i[1]\n",
    "\n",
    "    o=pd.Series(X_test[j])\n",
    "    o=vectorizer.transform(o)\n",
    "    o=scipy.sparse.csr_matrix.toarray(o)\n",
    "    o=o[0]\n",
    "\n",
    "    explainer = lime_tabular.LimeTabularExplainer(vtrain,mode=\"classification\",categorical_features=features,feature_names=features)\n",
    "    exp=explainer.explain_instance(data_row=o,predict_fn=mo.predict_proba, num_features=20) #labels=features\n",
    "    exp.as_pyplot_figure()\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.tight_layout()\n",
    "    exp.show_in_notebook()\n",
    "    return(j,exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(features)\\nwith plt.style.context(\"ggplot\"):\\n    fig = plt.figure(figsize=(8,6))\\n    plt.barh(range(len(mo.coef_[0])), mo.coef_[0], color=[\"red\" if coef<0 else \"green\" for coef in mo.coef_[0]])\\n    plt.yticks(range(len(mo.coef_[0])), features);\\n    plt.title(\"Weights\")\\n    '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(features)\n",
    "with plt.style.context(\"ggplot\"):\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.barh(range(len(mo.coef_[0])), mo.coef_[0], color=[\"red\" if coef<0 else \"green\" for coef in mo.coef_[0]])\n",
    "    plt.yticks(range(len(mo.coef_[0])), features);\n",
    "    plt.title(\"Weights\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def limenaivebayes(nbfake,nbtrue):\\n    data,lenfake,lentrue=preprocessing()\\n    \\n    x1=dataproc(data,nbfake,nbtrue,lenfake,lentrue)\\n    X_train=x1[0]\\n    X_test=x1[1]\\n    y_train=x1[2]\\n    y_test=x1[3]\\n    vtrain,vtest,vectorizer,features=vecto(X_train,X_test)\\n    #classifNaiveBayes(vtrain,y_train,vectorizer,X_test,features)\\n    j,exp=classifNaiveBayes(vtrain,y_train,vectorizer,X_test,features)\\n    #print(\\'Texte classé = \\',X_test[j])\\n\\n    outputs = open(\"outputs.txt\", \"w\")\\n    outputs.write(\"Texte classé = \")\\n    outputs.write(X_test[j])\\n    #outputs.write(exp)\\n    outputs.write(\"/n\")\\n    outputs.close()\\n    \\nlimenaivebayes(5,5)\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def limenaivebayes(nbfake,nbtrue):\n",
    "    data,lenfake,lentrue=preprocessing()\n",
    "    \n",
    "    x1=dataproc(data,nbfake,nbtrue,lenfake,lentrue)\n",
    "    X_train=x1[0]\n",
    "    X_test=x1[1]\n",
    "    y_train=x1[2]\n",
    "    y_test=x1[3]\n",
    "    vtrain,vtest,vectorizer,features=vecto(X_train,X_test)\n",
    "    #classifNaiveBayes(vtrain,y_train,vectorizer,X_test,features)\n",
    "    j,exp=classifNaiveBayes(vtrain,y_train,vectorizer,X_test,features)\n",
    "    #print('Texte classé = ',X_test[j])\n",
    "\n",
    "    outputs = open(\"outputs.txt\", \"w\")\n",
    "    outputs.write(\"Texte classé = \")\n",
    "    outputs.write(X_test[j])\n",
    "    #outputs.write(exp)\n",
    "    outputs.write(\"/n\")\n",
    "    outputs.close()\n",
    "    \n",
    "limenaivebayes(5,5)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news =  0        washington reuters head conservative republica...\n",
      "1        washington reuters transgender people allowed ...\n",
      "2        washington reuters special counsel investigati...\n",
      "3        washington reuters trump campaign adviser geor...\n",
      "4        seattlewashington reuters president donald tru...\n",
      "                               ...                        \n",
      "42829    brussels reuters nato allies tuesday welcomed ...\n",
      "42830    london reuters lexisnexis provider legal regul...\n",
      "42831    minsk reuters shadow disused sovietera factori...\n",
      "42832    moscow reuters vatican secretary state cardina...\n",
      "42833    jakarta reuters indonesia buy 11 sukhoi fighte...\n",
      "Name: text, Length: 42834, dtype: object\n",
      "0.064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npositive sentiment: compound score >= 0.05\\nneutral sentiment: (compound score > -0.05) and (compound score < 0.05)\\nnegative sentiment: compound score <= -0.05\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data=preprocessing()\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer=SentimentIntensityAnalyzer()\n",
    "print(\"news = \",data['text'])\n",
    "sentiment=[]\n",
    "for news in data['text'][0:5]:\n",
    "    x=analyzer.polarity_scores(news)['neg']\n",
    "    sentiment=sentiment+[x]\n",
    "print(sentiment[0])\n",
    "\n",
    "'''\n",
    "positive sentiment: compound score >= 0.05\n",
    "neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "negative sentiment: compound score <= -0.05\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52fd7cfe315652c26f116941759022c1154f5724bf85b7c7e9d121c272da0755"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
